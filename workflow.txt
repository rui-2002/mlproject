1. Create a new repo in github
2. Create a folder open that in terminal , launch it in vscode with help of code .
3.create a separate environment for it
4. conda create -p venv python==3.12 -y
5. conda activate venv/
6. Setup github so we can initialize commit
7. git init
8. Create a readme file : README.md 
9. Type: ## End to End Machine Learning Project
10. add readme file to github :git add README.md
11. add commit : git commit -m "First commit"
12. git branch -M main
13. git remote add origin https://github.com/rui-2002/mlproject.git
14. git remote -v (check link with github origin)
15. for first time -> add username and email

:git config --global user.name "rui-2002"
:git config --global user.email sangwansumit628@gmail.com

16. to push :git push -u origin main
17. In github add : .gitignore (file in python)
reason : some file that may not be commited in github get removed
18 to update from git : git pull
19.create file : setup.py and requirements.txt
setup.py create ml project as a package 


## put this in setup.py : meta-data info about all project (info about package)



##################################################################################################

from setuptools import find_packages,setup





setup(
    name='mlproject',
    version='0.0.1',
    author='Sumit',
    author_email='sangwansumit628@gmail.com',
    packages=find_packages(),
    install_requires=['pandas','numpy','seaborn']
    
)



##################################################################################################








## this packages method in setup will find packages
## setup.py will find out how many packages are there and where they are
## src(source) folder find out to be a package i.e by creating a file init i.e __init__.py


16. Create a folder in vscode src(source) : src
17. Create : __init__.py (file in src)

## whenver this setup.py (find package is running it will find how many folder do you have __init__.py file)
## now setup.py (will consider src as package)
## Note : to be a package it must contain __inti__.py file
## this setup.py will try to build the package and after building it we can import it anywhere we want.(like seabor,pandas)
## but for that we have to place it in pypy package itself.
## entire project building will happen in src folder.
## whenever we create new folder there will be using it as a package


## Note : but we req many package in setup (setup.py) , therefore we create a function


18. Whenver we try to install all the requirements.txt , then setup.py file should run to build the packages
# to enable that we specifically write : -e .(we should add it in requirements.txt and remove in setup.py)
# this will auto matically trigger setup.py


#################################################setup.py########################################
from setuptools import find_packages,setup
from typing import List


HYPEN_E_DOT='-e .'

def get_requirements(file_path:str)->List[str]:
    '''
    this function will return the list of requirements
    '''
    requirements=[]
    with open(file_path) as file_obj:
        requirements=file_obj.readlines()
        # replace slash(next line ) with blank
        requirements=[req.replace("\n"," ") for req in requirements]

        # remove '-e .' while installing package as it may cause error
        # only req to run setup.py
        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    return requirements


setup(
    name='mlproject',
    version='0.0.1',
    author='Sumit',
    author_email='sangwansumit628@gmail.com',
    packages=find_packages(),
    install_requires=get_requirements('requirements.txt')
    
)


###########################################################################################################################

19. pip install requirements.txt
(mlproject.egg-info is generated)


20. git add .
21. git status (new files which are added)
22. git commit -m "Second Commit "
23. git push -u origin main


24. Create component as a package i.e (created components in src and  create __init__.py)
## this components all the modules we are going to create (eg we create data ingestion i.e reading a dataset from database or other resource) 

25. After data ingestion we try differnt technique data transormation,validation etx
26. Creating data_transformation.py (one hot encoding,label encoding,missing data)
27. Create model_trainer.py (For training purpose,evalution matrix , also using for deployment i.e consist of seprate package but concluding it in this for now)

28. Create a new folder pipeline is src
pipeline will containe trainer pipeline (from trainer pipeline we will trigger/call components)and other is predict pipeline (for predicting purpose once model is created)
create __int__.py file so that we can import it

# as our entire project implementation will be happening inside the source
## we create 3 import files over here(for logging,exception,utils (any functionality we will be writing in common way which we will use in entire application))
 

 29. Create logger.py , utils.py , exception.py in src

 ## utils (read data from dataset,create mongodb client over here,write code to save  model in the code)
 




 -----------------------------------------------------------------------------------------------------
 # workflow 2 :

 1. lets start with data ingestion in components (splittin ,cross validation ,reading the data) after then only data ingestion will happen
 whenever we apply data ingestion some input is required for data ingestion(such as saving input path,path saving raw data) those type of input are created in different class as Dataingestion config class.
 for input we use Dataingestion config class  
 * To use this data ingestion class we need a decorator dataclass
 * Because inside a class , to define class variable we use init but if we try to use this dataclass we can able to define class variable

 2. To read data from the other dataset ,we can create mongodb/sql client in utils file and read in data ingestion.
 3. Always write log to see incase exception happen we can find it where it will happen.
 

 4. run data_ingestion file :python src/components/data_ingestion.py
 you will se artifacts folder will be created.

 5. create data_transformation file and link it with data ingestion and run it
 (run data_ingestion file)

 # to build a package with -e . in requirements simpliy run the requirements
 #  to again rebuild package we use -e .


 6. created model_trainer and write its common fn in utils.
 7. after complting model_trainer add this into data ingestion src.components.model_trainer import ModelTraineConfig,ModelTrainer.
 8. created flask app (app.py)
 9.created templates (index.html,index.html )
 10. created predict_pipeline.py
 